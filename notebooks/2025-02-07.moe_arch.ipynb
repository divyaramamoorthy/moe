{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoE Architecture - based on Deepseek-v3 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "    Expert layer for Mixture-of-Experts (MoE) models.\n",
    "    It looks like Deepseek modeled this off of Meta's Llama code: https://github.com/meta-llama/llama/blob/main/llama/model.py\n",
    "    It is a gated linear unit (GLU) that uses a SiLu (instead of sigmoid) activation function.\n",
    "    Orig GLU paper: https://arxiv.org/pdf/2002.05202\n",
    "\n",
    "    Attributes:\n",
    "        w1 (nn.Module): Linear layer for input-to-hidden transformation.\n",
    "        w2 (nn.Module): Linear layer for hidden-to-output transformation.\n",
    "        w3 (nn.Module): Additional linear layer for feature transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, inter_dim: int):\n",
    "        \"\"\"\n",
    "        Initializes the Expert layer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Input and output dimensionality.\n",
    "            inter_dim (int): Hidden layer dimensionality.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, inter_dim)\n",
    "        self.w2 = nn.Linear(inter_dim, dim)\n",
    "        self.w3 = nn.Linear(dim, inter_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Expert layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after expert computation.\n",
    "        \"\"\"\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, dim, n_experts, topk_experts):\n",
    "        \"\"\"\n",
    "        Gating mechanism for routing inputs in a mixture-of-experts (MoE) model.\n",
    "\n",
    "        Load balancing: Deepseek-v3 uses auxiliary-loss-free load balancing, which adds a \n",
    "        bias term for each experts that is added into the affinity scores prior to selecting\n",
    "        the topk; this is dynamically updated during training. If the expert is overloaded,\n",
    "        then those experts are scaled down by a constant; and vice versa. The authors \n",
    "        claim this does better than pure auxiliary loss!\n",
    "\n",
    "        The authors also use a complementary sequence-wise auxiliary loss; this further encourages tokens within a sequence to be balanced across experts.\n",
    "        They note that the contribution of this is small related to the auxiliary-loss-free load.\n",
    "        \n",
    "        Implements:\n",
    "        - topK gating\n",
    "        - auxiliary-loss-free load balancing\n",
    "\n",
    "        Does NOT implement:\n",
    "        - sequence-wise auxiliary loss: not present in pulic deepseek code, but mentioned in paper?\n",
    "        - group-wise routing (hierarchical routing)\n",
    "        - mixed-precision training\n",
    "        - shared experts\n",
    "        - route-scale\n",
    "        \n",
    "        Args:\n",
    "            dim (int): Input dimension\n",
    "            n_experts (int): Number of experts\n",
    "            topk_experts (int): Number of  experts activated for each input.\n",
    "            weight (torch.nn.Parameter): Learnable weights for the gate.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_experts = n_experts\n",
    "        self.topk_experts = topk_experts\n",
    "        self.weight = nn.Parameter(torch.empty(n_experts, dim))\n",
    "        self.bias = nn.Parameter(torch.empty(n_experts))\n",
    "        #Unlike the Deepseek paper, I am adding Xavier initialization\n",
    "        #Reminder: functions that end with _ indicate inplace operations\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        # Unlike the Deepseek paper, I am adding zero initialization\n",
    "        nn.init.zeros_(self.bias)\n",
    "        assert self.n_experts > 1, 'Number of experts must be greater than 1'\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        scores = F.linear(x, self.weight)\n",
    "        # Deepseek-v3 uses a sigmoid function to compute the affinity scores, and then applies normalization after experts are selected\n",
    "        scores = scores.sigmoid()\n",
    "        original_scores = scores\n",
    "        #Bias term ONLY used to influence topK selection\n",
    "        scores = scores + self.bias\n",
    "        indices = torch.topk(scores, self.topk_experts, dim=-1)[1]\n",
    "        weights = original_scores.gather(1, indices)\n",
    "        weights /= weights.sum(dim=-1, keepdim=True) #Normalize to a probabilty vector (sum to 1)\n",
    "        return weights, indices\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture-of-Experts (MoE) module.\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): Dimensionality of input features.\n",
    "        n_experts (int): Total number of experts in the model.\n",
    "        n_activated_experts (int): Number of experts activated for each input.\n",
    "        experts (nn.ModuleList): List of expert modules.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_experts = n_experts\n",
    "        self.n_activated_experts = n_activated_experts\n",
    "        # self.experts = nn.ModuleList([Expert(dim, moe_inter_dim) for _ in range(n_experts)])\n",
    "        # self.gate = Gate(dim, n_experts)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5641, 0.4359],\n",
      "        [0.5179, 0.4821],\n",
      "        [0.5052, 0.4948],\n",
      "        [0.5439, 0.4561],\n",
      "        [0.5055, 0.4945],\n",
      "        [0.5003, 0.4997],\n",
      "        [0.5230, 0.4770],\n",
      "        [0.5349, 0.4651],\n",
      "        [0.5326, 0.4674],\n",
      "        [0.5061, 0.4939]], grad_fn=<DivBackward0>)\n",
      "tensor([[5, 1],\n",
      "        [5, 2],\n",
      "        [7, 0],\n",
      "        [1, 9],\n",
      "        [9, 7],\n",
      "        [3, 5],\n",
      "        [7, 1],\n",
      "        [8, 0],\n",
      "        [9, 3],\n",
      "        [8, 3]])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "# test the gate\n",
    "gate = Gate(dim=100, n_experts=10, topk_experts=2)\n",
    "\n",
    "x = torch.randn(10, 100) #Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1\n",
    "weights, indices = gate(x)\n",
    "print(weights)\n",
    "print(indices)\n",
    "\n",
    "# test the expert\n",
    "expert = Expert(dim=100, inter_dim=10)\n",
    "print(x.shape)\n",
    "y = expert(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/model.py\n",
    "class Gate(nn.Module):\n",
    "    \"\"\"\n",
    "    Gating mechanism for routing inputs in a mixture-of-experts (MoE) model.\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): Dimensionality of input features.\n",
    "        topk (int): Number of top experts activated for each input.\n",
    "        n_groups (int): Number of groups for routing.\n",
    "        topk_groups (int): Number of groups to route inputs to.\n",
    "        score_func (str): Scoring function ('softmax' or 'sigmoid').\n",
    "        route_scale (float): Scaling factor for routing weights.\n",
    "        weight (torch.nn.Parameter): Learnable weights for the gate.\n",
    "        bias (Optional[torch.nn.Parameter]): Optional bias term for the gate.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initializes the Gate module.\n",
    "\n",
    "        Args:\n",
    "            args (ModelArgs): Model arguments containing gating parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = args.dim\n",
    "        self.topk = args.n_activated_experts\n",
    "        self.n_groups = args.n_expert_groups\n",
    "        self.topk_groups = args.n_limited_groups\n",
    "        self.score_func = args.score_func\n",
    "        self.route_scale = args.route_scale\n",
    "        self.weight = nn.Parameter(torch.empty(args.n_routed_experts, args.dim))\n",
    "        self.bias = nn.Parameter(torch.empty(args.n_routed_experts)) if self.dim == 7168 else None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for the gating mechanism.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Routing weights and selected expert indices.\n",
    "        \"\"\"\n",
    "        scores = linear(x, self.weight)\n",
    "        if self.score_func == \"softmax\":\n",
    "            scores = scores.softmax(dim=-1, dtype=torch.float32)\n",
    "        else:\n",
    "            scores = scores.sigmoid()\n",
    "        original_scores = scores\n",
    "        if self.bias is not None:\n",
    "            scores = scores + self.bias\n",
    "        if self.n_groups > 1:\n",
    "            scores = scores.view(x.size(0), self.n_groups, -1)\n",
    "            if self.bias is None:\n",
    "                group_scores = scores.amax(dim=-1)\n",
    "            else:\n",
    "                group_scores = scores.topk(2, dim=-1)[0].sum(dim=-1)\n",
    "            indices = group_scores.topk(self.topk_groups, dim=-1)[1]\n",
    "            mask = torch.zeros_like(scores[..., 0]).scatter_(1, indices, True)\n",
    "            scores = (scores * mask.unsqueeze(-1)).flatten(1)\n",
    "        indices = torch.topk(scores, self.topk, dim=-1)[1]\n",
    "        weights = original_scores.gather(1, indices)\n",
    "        if self.score_func == \"sigmoid\":\n",
    "            weights /= weights.sum(dim=-1, keepdim=True)\n",
    "        weights *= self.route_scale\n",
    "        return weights.type_as(x), indices\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "    Expert layer for Mixture-of-Experts (MoE) models.\n",
    "\n",
    "    Attributes:\n",
    "        w1 (nn.Module): Linear layer for input-to-hidden transformation.\n",
    "        w2 (nn.Module): Linear layer for hidden-to-output transformation.\n",
    "        w3 (nn.Module): Additional linear layer for feature transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, inter_dim: int):\n",
    "        \"\"\"\n",
    "        Initializes the Expert layer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Input and output dimensionality.\n",
    "            inter_dim (int): Hidden layer dimensionality.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w1 = Linear(dim, inter_dim)\n",
    "        self.w2 = Linear(inter_dim, dim)\n",
    "        self.w3 = Linear(dim, inter_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Expert layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after expert computation.\n",
    "        \"\"\"\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture-of-Experts (MoE) module.\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): Dimensionality of input features.\n",
    "        n_routed_experts (int): Total number of experts in the model.\n",
    "        n_local_experts (int): Number of experts handled locally in distributed systems.\n",
    "        n_activated_experts (int): Number of experts activated for each input.\n",
    "        gate (nn.Module): Gating mechanism to route inputs to experts.\n",
    "        experts (nn.ModuleList): List of expert modules.\n",
    "        shared_experts (nn.Module): Shared experts applied to all inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initializes the MoE module.\n",
    "\n",
    "        Args:\n",
    "            args (ModelArgs): Model arguments containing MoE parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = args.dim\n",
    "        assert args.n_routed_experts % world_size == 0, f\"Number of experts must be divisible by world size (world_size={world_size})\"\n",
    "        self.n_routed_experts = args.n_routed_experts\n",
    "        self.n_local_experts = args.n_routed_experts // world_size\n",
    "        self.n_activated_experts = args.n_activated_experts\n",
    "        self.experts_start_idx = rank * self.n_local_experts\n",
    "        self.experts_end_idx = self.experts_start_idx + self.n_local_experts\n",
    "        self.gate = Gate(args)\n",
    "        self.experts = nn.ModuleList([Expert(args.dim, args.moe_inter_dim) if self.experts_start_idx <= i < self.experts_end_idx else None\n",
    "                                      for i in range(self.n_routed_experts)])\n",
    "        self.shared_experts = MLP(args.dim, args.n_shared_experts * args.moe_inter_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the MoE module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after expert routing and computation.\n",
    "        \"\"\"\n",
    "        shape = x.size()\n",
    "        x = x.view(-1, self.dim)\n",
    "        weights, indices = self.gate(x)\n",
    "        y = torch.zeros_like(x)\n",
    "        counts = torch.bincount(indices.flatten(), minlength=self.n_routed_experts).tolist()\n",
    "        for i in range(self.experts_start_idx, self.experts_end_idx):\n",
    "            if counts[i] == 0:\n",
    "                continue\n",
    "            expert = self.experts[i]\n",
    "            idx, top = torch.where(indices == i)\n",
    "            y[idx] += expert(x[idx]) * weights[idx, top, None]\n",
    "        z = self.shared_experts(x)\n",
    "        if world_size > 1:\n",
    "            dist.all_reduce(y)\n",
    "        return (y + z).view(shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
